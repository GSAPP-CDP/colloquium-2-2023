---
student: Yilin Wang
layout: blank.liquid
tags:
- students
image: final_review/cover image.png
---

<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">  
  <meta name="description"
  content="Student work from Colloquium 1 of the Columbia GSAPP CDP Program in 2023." />
  <link rel="icon" href="https://gsapp-cdp.github.io/colloquium-1-2023/favicon.ico" sizes="any">
  <link rel="icon" href="https://gsapp-cdp.github.io/colloquium-1-2023/favicon.svg" type="image/svg+xml">
  <link href="https://gsapp-cdp.github.io/colloquium-1-2023/base.css" rel="stylesheet" />
  <link href="https://gsapp-cdp.github.io/colloquium-1-2023/main.css" rel="stylesheet" />
  <link href="https://gsapp-cdp.github.io/colloquium-1-2023/colloquium1.css" rel="stylesheet" />

  <link href="mystyle.css" rel="stylesheet" />

  <title>Yilin Wang</title>
</head>

<body>
  <header class="container">
    <div class="header-context">
      <div class="h2">
        <a class="fade" href="https://www.arch.columbia.edu/programs/15-m-s-computational-design-practices"
          target="_blank">Columbia GSAPP</a>
      </div>
    </div>

    <div class="header-title">
      <div class="h1">
        <a class="fade" href="/">Computational Design Practice</a>
      </div>
      <div class="h2">GSAPP CDP 2023-4 Colloquium II</div>
    </div>

    <div class="header-nav">
      <div class="h2">
        <a class="fade" href="/about">About</a>
      </div>
    </div>
  </header>

  <main>
    <div class="container">

      <div class="post">
        <div class="post-header">
          <div class="post-title">
            <h1>Yilin Wang</h1>
          </div>
        </div>

        <div class="post-body project content">
          <!-- ALL WORK SHOULD BE CONTAINED HERE. -->

          <div class="toc full">
            <h2>[ Urban-Climate GAN ]
            </h2>
            <ul>
              <li><a > Final Review of Colloquium2: Explore/ Explain/ Propose</a></li>
              <!-- <li><a href="#assignment-2">2 &ndash; Idea2</a></li> -->
              <!-- <li><a href="#assignment-3">3 &ndash; Idea3</a></li> -->

            </ul>
          </div>

          <div class="full assignment">
            <h1 id="assignment-1">1 &ndash; Explore</h1>
            <h2>Urban 3D Morphology prediction model adaptive to local climate zones(lcz) classification
            </h2>
            
          </div>

          <div class="full">
            <h2>Explore/ Experience</h2>
            <p>
              At the first few weeks of this semester, I explored on issues how AI plays the role as a tool in Computational
              Design. And I have tried some experienment on different aspects of this issue.
            </p>

          </div>

          <div class="half">
            <h2>Experience on combining AI with design / data visualziation           </h2>
            <p>
              The first experience is combining parametric design with stable diffusion rendering. Stable Diffusion Webui
              is originally a text/image to image algorithm, which would be great for architectural designers to make renderings
              much more conveniently. I combine this AI part with grasshopper parametric design, so as to make renders interactively
              and conveniently as parametric architectural model changes to every option.
            </p>

            <p>
              The second experience is to integrate advanced Computer Vision technologies with data visualization in javascript
              /mapbox. The method for creating dataset is to firstly utilize google api to download street view images and then employ
              computer vision to make semantic segmentation for each image, and finally calculate each components' ratio in images.


            </p>
          </div>

          

          <div class="half">
            <video  controls>
              <source src="final_review/final_presentation_short.mp4" type="video/mp4">
              您的浏览器不支持视频标签。
            </video>
            
          </div>

          <div class="half">
            <img src="final_review/SD1.png">
          </div>

          <div class="half">
            <img src="final_review/SD2.png">
          </div>

          <div class="half">
            <h2>Research on GAN (pix2pix) algorithm           </h2>
            <p>
              Generative Adversarial Networks (GANs) are a type of deep learning model consisting of two parts: 
              a generator and a discriminator. The generator is responsible for creating realistic-looking data, 
              while the discriminator's job is to distinguish between data produced by the generator and real data. 
              Through this adversarial process, the generator learns to produce increasingly realistic data.

            </p>

            <p>
              Pix2pix is a specialized form of GAN designed for paired image-to-image translation. It learns to 
              transform an input image into an output image through training, making it suitable for tasks like style 
              transfer, colorization, and more. Pix2pix trains with paired input and output images, enabling it to 
              learn how to convert one type of image into another.



            </p>
          </div>

          <div class="half">
            <img src="final_review/pix2pix diagram.jpg">
            
          </div>

          <div class="full">
            <img src="final_review/pix2pix(1).png">
            
          </div>
          

          <div class="full">
            
            <h2>Existing Research on GAN as a tool for city planning</h2>
            <p>
              Here is a piece of paper on how to use GAN in urban planning. The first one is to utilize GAN to generate imagined
              building footprint for blocks and then designers make 3D model based on this. The paired input and 
              output images are empty block boundaries and the ground truth building footprint in respective blocks.

            </p>
            
          </div>

          <div class="half">
            <img src="final_review/R1.1.png">
          </div>

          <div class="half">
            <img src="final_review/R1.2.png">
          </div>

          <div class="half">
            
            <h2>Local Climate Zone(LCZ) in micro climate analysis of urban planning            </h2>
            <p>
              Urban has endured severe micro-climate situation such as urban heat island, urban heat wave, and flooding.

            </p>
            <p>
              Local Climate Zones (LCZ) maps play a crucial role in urban planning by providing detailed 
              classifications of urban and natural environments based on thermal and physical properties. These maps 
              facilitate the understanding of microclimates within cities, aiding in designing more sustainable and 
              climate-resilient urban spaces. 

            </p>
            <p>
              By categorizing areas into distinct climate zones, LCZ maps help urban planners and environmental 
              researchers to assess the impact of urban design on local temperature, air quality, and energy 
              consumption. This data-driven approach enables informed decision-making for urban development, 
              prioritizing both environmental sustainability and the well-being of urban inhabitants.

            </p>
            

            
          </div>

          <div class="half">
            <img src="final_review/lcz1.png">
          </div>

          <div class="half">
            <img src="final_review/lcz2.jpg">
          </div>

          <div class="half">
            <img src="final_review/lcz3.png">
          </div>


          <div class="full">
            
            <h2>Research / Experience on current condition of computational urban design/ parametric design
            </h2>
            <p>
              While micro-climate plays an significant role in adaptive urban planning, in the following two experiences I have done in normal computational design in architecture/ urban planning, 
              it is often considered as metric analysis for designed option instead of input parameter that 
              designers take into account at the beginning of design process.
            </p>  
          </div>


          <div class="half">
            <img src="final_review/cd_1.gif">
          </div>

          <div class="half">
            <video  controls>
              <source src="final_review/prototype_How_to_Scout_short.mp4" type="video/mp4">
              您的浏览器不支持视频标签。
            </video>
            
          </div>

          <div class="full">
            
            <h2>Research Question: Generating urban 3D morphology that rapidly responds to LCZ changes? 
            </h2>
            <p>
              Current computational urban design primarily involves the generation of 3D models based on various 
              parameters (street width, block shape, FAR, building types), followed by environmental analysis based 
              on these models.

            </p>  
            <p>
              Meanwhile, AI research in the field of building environment has been mainly focused on creating 
              two-dimensional images for visual presentations or further data visualization. 


            </p> 
            <p>
              Therefore, the research question for my project is: Can designers generate urban 3D morphology models 
              using urban microclimate data(lcz maps) as parameters? Is it possible to utilize lcz maps as a design 
              guide, so that when the color classification of lcz maps change, the urban morphology change responsively?


            </p> 
            <p>
              The primary goal of the project is to utilize LCZ maps for predicting and modifying urban 3D morphology 
              models. This innovation has the potential to influence urban planning/ design by enabling more dynamic 
              adaptations to the local climate, thereby fostering more sustainable and responsive urban environments.

            </p> 
          </div>

          <div class="full assignment">
            <h1 id="assignment-1">2 &ndash; Explain</h1>
            <h2>Explain a design framework for generating urban 3D morphology that rapidly responds to local climate zones(lcz) changes
            </h2>
            
          </div>

          <div class="full">
            
            <h2>Overall Methodologies 
            </h2>
            <p>
              The following diagram demonstrates the overall methodologies for my capstone project.Basically it can be
              conceived as two parts. The first part is to set up AI -parametric design workflow, the second part is to
              utilize this workflow to generate new design options.

            </p>  
            <p>
              The first part consists of two procedures: pix2pix and then parametric design. The paired input and output
              images for pix2pix part are lcz classification mapping and respective scale, ground truth morphology mapping.
              By combining this two datasets to pix2pix algorithm and training the AI model, we can generate new urban
              morphology mapping based on new input of lcz classification mapping. Then these generated urban morphology mapping
              will be imported into grasshopper to make parametric model for corresponding urban blocks.

            </p> 
            <p>
              The second part is to adapt to some changes on existing lcz and road network so that simultaneously urban 3d model
              will make responsive changes based on lcz data changes. In this case, urban 3d model will display dynamic response
              with lcz mapping.


            </p> 
          </div>

          <div class="full">
            <img src="final_review/overall_methodologies.png">
          </div>


          <div class="full">
            
            <h2>Data Source -- Ground Truth Urban Morphology Mapping 
            </h2>
            <p>
              As introduced before, the dataset for pix2pix model consists of paired input and output images.

            </p>  
            <p>
              The output images for training the model are ground truth urban morphology mapping. I got city footprint 
              from mapbox street tile v8 and make color coded for water and greenery. From OSM buildings I acquired 
              geojson of buildings containing information of each building's height. Finally, I joined these height information
              with building footprint and color coded them by red color. Since it requires api to get more access to height
              information on OSM building and api is quite expensive, for this semester I only acquire New York City's and
              Los Angeles's city center part building height information.

            </p>  
          </div>

          <div class="half">
            <img src="final_review/nyc_urban_mapping.png">
          </div>

          <div class="half">
            <img src="final_review/LA_urban_mapping.png">
          </div>

          <div class="full">
           
            <p>
              The input images for training the model are lcz classification mappings. I get access to some existing
              lcz mapping on World Urban Database and Access Portal Tools (WUDAPT). The resolution of these data are 
              30m, which is detailed enough to distinguish building footprint in the scale of city blocks. I have chosen
              some metropolis to analyze lcz mapping since the mega city could have as many classifications as possible.
              Plus, I chose main cities' central area for analysis. 

            </p>  
          </div>

          <div class="half">
            <img src="final_review/lcz_mapping1.png">
          </div>

          <div class="half">
            <img src="final_review/lcz_mapping2.png">
          </div>


          <div class="full assignment">
            <h1 id="assignment-1">3 &ndash; Propose</h1>
            <h2>Make a prototype and list final deliverables for next semester
            </h2>
            
          </div>

          <div class="full">
            
            <h2>Make a prototype for the capstone project
            </h2>
            <p>
              During this semester, I have made a prototype for preparing datasets and training pix2pix models. Their
              position in overall methodologies is displayed below.

            </p>  
            
          </div>

          <div class="full">
            <img src="final_review/prototype_order.png">
          </div>

          <div class="full">
            
            <h2>MVP for pix2pix part
            </h2>
            <p>
              I have chose data of city center of New York City and Los Angeles. I have exported from QGIS the paired
              images of lcz mapping+ road network / urban morphology mapping with building information color coded. The
              scale of these paired images have the exact same location and dimension, and the scale is roughly 1km* 1km.
              

            </p>  
            
          </div>

          <div class="full">
            <img src="final_review/prototype_dataset1.png">
            <img src="final_review/prototype_dataset2.png">
          </div>

          <div class="half">
            
            <h2>Training and testing pix2pix model
            </h2>
            <p>
              Now the dataset contains 80 paired input and output images of New York City's and Los Angeles's central area
              lcz mapping and urban morphology mapping. For the prototype of pix2pix model, I have randomly selected
              80% of paired data for training the model; 15% for val; 5% for testing data.

            </p> 
            
            <p>
              The paired images on the right demonstrates two results among data for testing the model. The order of
              these images from left to right is: existing lcz mapping; ground truth urban morphology mapping and
              generated/faked new urban morphology mapping.

            </p> 
            
          </div>

          <div class="half">
            <img src="final_review/prototype_testing1.png">
          </div>

          <div class="half">
            
            <h2>List for final deliverables
            </h2>
            <p>
              The first part is to gather and preprocess a whole dataset. As I introduced before, the lcz data I acquired
              for this semester is from website and they are generated by others. For next semester, I am supposed to utilize
              lcz generator to create my own data for lcz mapping.

            </p> 
            
            <p>
              Another important prototype for final deliverable is setting up grasshopper custom tool that generated 3d model
              based on imagined 3d urban morphology mapping from pix2pix model.

            </p> 

            <p>
              A thorough documentation including paired images dataset, responsive 3d model, dynamic response of models,
              website/video displaying the whole workflow will be delivered next semester.

            </p> 
            
          </div>

          <div class="half">
            <img src="final_review/lcz generator1.jpg">
          </div>

          <div class="full">
            <img src="final_review/deliverables.png">
          </div>

          <div class="full">
            
            <h2>Bibliography
            </h2>
            <p>
              https://github.com/SerjoschDuering/StableDiffusion-Grasshopper    github repo on how to set up stable diffusion in grasshopper
https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix    github repo on what is pix2pix and how to utilize pix2pix algorithm
https://journals.sagepub.com/doi/full/10.1177/23998083221100550    a paper on integrating GAN to urban design
https://link.springer.com/chapter/10.1007/978-981-33-4400-6_10#Fig2   a paper on integrating GAN to urban design
https://www.frontiersin.org/articles/10.3389/fenvs.2021.637455/full    a paper on how to use LCZ Generator to generate lcz maps on one’s own
https://www.wudapt.org/lcz-maps/    World Urban Database and Access Portal Tools, the website where to access lcz data and lcz generator
https://scout.kpfui.dev/?project=tampa    a case study of KPFui on current computational urban design


            </p> 
            
            <h2>Data Source
            </h2>
            <p>
              https://docs.mapbox.com/  mapbox style setup and edit customize styles
https://osmbuildings.org/  using this website api can get most buildings’ height information in the world


            </p> 
            
          </div>

          

          

          <!-- <div class="full assignment">
            <h1 id="assignment-2">2 &ndash; Idea2</h1>
            <h2>Generate cities from cities</h2>
            <p></p>
          </div>

          <div class="full">
            <h2>Research Question</h2>
            <p>
              How can the pix2pix GAN, trained on urban satellite imagery, assist designers in generating 
              prospective urban layouts for areas under development or transformation?
            </p>
            
            <h2>Computational tools and design methods</h2>
            <p>
              Data Collection & Preprocessing: Utilizing urban satellite imagery, specifically focusing on the patterns of differnet varieties of 
              urban spaces.

            </p>
            <p>
              ML - pix2pix: Implementing and training a pix2pix GAN that learns the characteristics of these urban spaces
              from the collected images.
            </p>

            <p>
              Design Generation & Visualization(in GH): Applying the trained model to images of areas under development, generating and visualizing 
              potential urban layouts that follow the learned patterns.
            </p> 
          </div>


          <div class="half">
            <img src="1005/i2_pix2pix.png">
            <img src="1005/i2_city_scope1.png">
            <img src="1005/i2-city_scope2.png">
            
            
          </div>

        

          <div class="half">
            <h2>Related Precedents</h2>

            <p>
              pix2pix Projects: Existing applications of pix2pix in image-to-image translation tasks, particularly those related to spatial or 
              architectural design. These GAN algorithms nowadays are quite mature.
            </p>

            <p>
              MIT media lab: The Deep Image of the City: the project utilizes GANs to create deep images of cities, 
              which are essentially synthesized representations of urban areas that don't necessarily exist in reality but are plausible 
              based on the learned patterns and features from real urban imagery. 
            </p>

            <h2>Data for Exploration</h2>

            <p>
              Collection of diverse urban satellite images, preferably annotated or categorized based on the urban planning variety(for example, the road, natural condition), 
              to train the pix2pix model effectively.
            </p>

            <h2>Project Scales</h2>
            <p>
              While initial tests may focus on specific districts or areas within cities, the methodology can be also applied to villages(for example, traditional villages in China)
            </p>

            <h2>Sketches and Audiences</h2>
            <p>
              Sketches will depict the workflow from satellite image input, pix2pix processing, to generated urban layout output and visualize the 2d images in 3d way.
              The target audience includes urban designers, architects, and city planners as well as public.
            </p>

            <img src="1005/i2-sketch1.png">
            <img src="1005/i2_sketch2.png">
            <img src="1005/i2_sketch3.jpg">


          </div>

          <div class="half">
            <img src="">
          </div>

        

          <div class="full assignment">
            <h1 id="assignment-3">3 &ndash; </h1>Idea3
            <h2> Research Question: Can we develop architectural elements that adapt in real-time to environmental changes, using Machine Learning 
              and environmental sensors?</h2>
            <p></p>
          </div>

          <hr>

          <div class="half">
            <img src="">
          </div>

          <div class="half">
            <img src="">
          </div>

          <hr>

          <div class="full">
            <h2>Computational Tools and Design Methods</h2>

            <p>
              Data Collection & Preprocessing: Implement environmental sensors to gather real-time data on factors like temperature, humidity, 
              light intensity, and air quality.
            </p>

            <p>
              Responsive Architectural Elements: Develop elements like dynamic facades, adjustable louvers, or smart windows that react to the 
              analyzed data, adapting their configurations to maintain interior comfort and energy efficiency.
            </p>

          

          </div>

          

          <div class="half">
            <img src="0927//I3-Autodesk.png">
            <img src="0927//I1-Autodesk.png">


    

          </div>

          

          <div class="half">
            <h2>Data for Exploration</h2>
            <p>
              Sensors will continuously gather environmental data. Ensure accessibility to this data stream for analysis and response triggering 
              in the architectural elements.
            </p>
            <h2>Project Scale</h2>
            <p>
              The project can be applicable to single architectural elements or even entire building facades,etc.
              
            </p>

            <h2>related precedents</h2>
            <p>
              Autodesk Project Dasher employs environmental sensors to gather continuous data on various parameters like 
              temperature, humidity, occupancy, and energy consumption. This sensor-derived data is then dynamically mapped onto the building's 3D 
              BIM model. Through this integration, Project Dasher provides a comprehensive, real-time overview of the building's performance and 
              environmental conditions, enabling architects, engineers, and facility managers to understand and optimize the operation of buildings 
              effectively.
              
            </p>
          </div> -->

          

        </div>
      </div>

      <footer></footer>
    </div>
  </main>
</body>

</html>

